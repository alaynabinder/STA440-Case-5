---
title: "Stroke Outcomes Analysis Report"
author: "Dom Fenoglio, Sonya Eason, Alayna Binder"
date: "December 4, 2025"
output: 
  bookdown::pdf_document2:
    number_sections: true
    toc: false
---

<!-- NOTE: To reference a figure in the report body, do @ref(fig:NameOfFig). -->

# Background and Data

Stroke is a time-sensitive emergency and a leading cause of death and long-term disability in the United States. Because symptoms are often subtle and painless, many patients present late, limiting the effectiveness of reperfusion therapies such as thrombolytics (TPA) and mechanical thrombectomy.

This report analyzes data from a two-year regional improvement program aimed at reducing delays in acute stroke care. Program initiatives focused on enhancing stroke recognition by the public and providers, strengthening EMS assessment and prehospital notification, and streamlining pathways among emergency departments, neurology consultants, and comprehensive stroke centers to accelerate treatment decisions.

The study includes a baseline period, an implementation period overlapping with the COVID-19 pandemic, and a final period after full program rollout. We examine ischemic stroke patients who received TPA, mechanical thrombectomy, or both, with goals of identifying factors associated with favorable discharge outcomes, evaluating outcome changes over time, and addressing substantial missing data in key covariates.

## Exploratory Data Analysis

We began by examining patterns of missingness across key variables. As shown in Figure @ref(fig:eda1), four variables had missingness: `Age`, `EMSvsCar`, `PreHospNotify`, and `Race2`, with age missing in about half of all records. Missingness for all variables was similar across outcome groups, indicating little evidence of outcome-dependent mechanisms. To assess site and time structure, we summarized missingness by site and study quarter in Figure @ref(fig:eda2). `Age` showed clear site-specific clustering, particularly at sites 140, 150, and 170, while `EMSvsCar`, `PreHospNotify`, and `Race2` showed smaller but visible site-level differences. No variable displayed a consistent temporal trend across sites. These structured patterns, especially concentrated gaps within specific sites, argue against a missing completely at random mechanism (MCAR), since MCAR would not generate the site-level heterogeneity observed in Figures @ref(fig:eda1) and @ref(fig:eda2). Instead, the data likely follow more complex missingness processes that must be addressed in the imputation strategy.

In our initial approach, we utillized MICE to output 20 imputed datasets. We separately imputed three variables (`Age`, `EMSvsCar`, and `PreHospNotify`, as the `Race2` announcement was not made yet). The process involved first imputing age via predictive mean matching, one of the default setting in MICE and then imputing `EMSvsCar` and then `PreHospNotify` by a logistic regression mechanism. In this process, we had to enforce the interrelatedness constraints that existed in `PreHospNotify` and `EMSvsCar` so we adopted a post-processing step where after imputation was done, we set `PreHospNotify` = "No" whenever `EMSvsCar` = 1. To strengthen our approach, we later decided to instead create a new variable rather than do separate imputation of these two variables, which we describe subsequently in modeling rationale. 

<!-- TIME-/SPACE- PERMITTING: SENSITIVITY ANALYSIS -->

Figure @ref(fig:eda3) shows clear unadjusted patterns across predictors. Younger patients have much higher favorable discharge rates, while outcomes decline steadily with age. Markers of greater severity such as EMS arrival, thrombectomy, and thrombectomy complications are associated with lower favorable discharge. Other predictors including gender and race show little variation.

### Preliminary frequentist model

After initially imputing 20 new data sets using mice, we ran a frequentist logistic regression model on each one to examine potential relationships. We saw similar coefficient estimates (Table @ref(tab:freq3)) and AUC values across all 20 imputed data sets (Table @ref(tab:freq4)), suggesting that mice effectively sampled from the correct distribution for our missing data. The most significant predictor by far was age, which was strongly tied to a negative impact on the log odds of a home or rehab outcome. Other significant predictors included race being African American, EMS vs. Car, and TPA complications. To examine model fit, we also looked at binned residual plots for each fitted model (Figure @ref(fig:freq1)). This separated the fitted values into groups based on their expected probability of successful outcome, then calculated the average residual in each group. The residual plot was acceptable, but showed some systematic trends that could be tied to the age variable.

# Our Model

## Model rationale

To model our binary outcome which represents positive or negative outcome with $Y_i = 1$ representing going to home or rehab, we adopted a Bayesian logistic regression. We fit a model where every covariate in the model had a simple relationship with the logistic link, but we did choose to incorporate a random intercept for sites due to independence concerns. To have independent observations, we believed it necessary to account for the random noise that the site introduced. An important component in the modeling process was tackling the missing data. In fact, our model also incorporated imputation for missing age, race, and transport/notification data.  

We represent our final logistic model as $\text{logit}(p_i) = \beta_0 + \mathbf{X}_i^\top \boldsymbol{\beta} + \beta_{\text{age}} \cdot \text{age}_i + \beta_{\text{race}, \text{race}_i} + \beta_{\text{tr}, \text{tr}_i} + \beta_{\text{time}, \text{time}_i} + b_{\text{site}, \text{site}_i}$, where $\mathbf{X}_i$ captures other fixed covariates (like Gender), $\text{race}_i \in \{1,\dots,K_{\text{race}}\}$, $\text{tr}_i \in \{1,\dots,K_{\text{tr}}\}$, and $\text{time}_i \in \{1,\dots,K_{\text{time}}\}$ are categorical indices with baselines fixed at zero ($\beta_{\text{race},1} = \beta_{\text{tr},1} = \beta_{\text{time},1} = 0$), site random effects follow $b_{\text{site},s} \sim \mathcal{N}(0, \tau_{\text{home}}^{-1})$, and all fixed effects use vague priors $\beta_0, \boldsymbol{\beta}, \beta_{\text{age}}, \beta_{\text{race},k \ge 2}, \beta_{\text{tr},k \ge 2}, \beta_{\text{time},k \ge 2} \sim \mathcal{N}(0,1000)$ with $\tau_{\text{home}} \sim \text{Gamma}(0.01,0.01)$. The priors were chosen to allow the data inform the parameters. 

We adopt a modeling pipeline to address missingness in age, race, EMSvsCar, and PreHospNotify. Our first consideration was the inherent relatedness of the EMSvsCar and the PreHospNotify variables. In fact, we see these variables are related because if someone rode in a car, there couldn't be a prehospital notification. For this reason, we created a new variable called transportNotify that had three levels representing the possible combinations of these two variables. From here, we imputed this covariate.

Both transportNotify and age were imputed to rely on the non-missing features in the data as well as the random effects for site. We present transportNotify and age as categorical and normally distributed variables repsectively and set noninformative priors. Specifiically, for age we use the priors $\beta_{0,\text{age}} \sim \mathcal{N}(0,1000)$, $\tau_{\text{age}}, \tau_{b,\text{age}} \sim \text{Gamma}(0.01,0.01)$. For, the categorical transportNotify, we adopt $\text{Dirichlet}(\mathbf{1})$ priors. These priors were chosen to allow for data to inform the imputation. 

Race also necessitated imputation, and our group decided to follow the instruction of imputing race based on iste. We adopted dirichlet-categorical imputation where $\boldsymbol{\pi}_{\text{race}} \sim \text{Dirichlet}(\mathbf{1}_{K_{\text{race}}})$ so $\text{race}_i \sim \text{Categorical}(\boldsymbol{\pi}_{\text{race}})$. 

One challenge in coding the JAGs model was ensuring cateogrical variables has baselines set, which was done to ensure reasonable interpretability of effects. 

## Model implementation

<!-- TO-DO -->

## Model evaluation

To examine convergence of our sampler, we examined traceplots (Figures XX), effective sample size (Table XX), and Rhat values (Table XX) for our parameters. Convergence diagnostics were strong for all parameters except for age, which is likely due to the fact that age was entirely missing for certain hospitals. Since we are imputing these age values each iteration of our sampler, there is a lot of variability and sensitivity in age. Still, the effective sample size for age is around 290 and we felt confident enough in our convergence to interpret results.

We also computed a binned residual plot again to compare it to our frequentist fit and examine model assumptions (Figure XX). We saw improvement in the residual plot in terms of randomness compared to our frequentist model, as there does not appear to be a trend in the residuals or any heteroskedascticity.

## Model results

Table @ref(tag:bayes1) shows that the Bayesian model reaffirms the same core associations observed previously. Figure @ref(fig:bayes2) visualizes these effects on the odds ratio scale. Age shows the clearest effect, with a negative coefficient whose 95 percent credible interval lies entirely below zero, indicating a sharp decline in the odds of a favorable discharge as age increases. Female sex, thrombectomy, and thrombolysis complications likewise have posterior odds ratios credibly below one, reinforcing their consistent negative associations. In contrast, the TPA without complications group shows an odds ratio clearly above one, providing strong evidence of benefit and remaining the only treatment related factor with a reliably positive effect.

The time indicators exhibit partial overlap with one, suggesting uncertain but directionally positive improvement in outcomes across the program periods. The median effects increase across time, and while the credible intervals include values near one, the posterior still points toward slightly better outcomes later in the study.

The Bayesian results closely parallel those from the frequentist model: almost all effect directions and substantive conclusions agree. The main difference is the narrower posterior credible intervals, which is expected because the Bayesian approach incorporates prior structure that stabilizes estimation and reduces uncertainty when the data strongly support a given effect.

# Shortcomings and Future Work

A major shortcoming of our model was the low effective sample size for age, which while likely tied to complete missing blocks of age for certain hospitals, decreases the confidence in our results, especially considering that age appears to be such a major factor in stroke outcomes. Future work should include examining alternate methods of imputing age, performing sensitivity analysis on the choice of priors, and possibly transforming age in our final logistic model.

Another shortcoming of our analysis is the inability to determine whether missing data had some dependence on a latent, unobserved variable. For example, it is possible that geographic location or insurance-status of each hospital was related to the age of its patients in some way. So, future work may also include collecting additional information on each hospital or repeating the experiment to limit the amount of data missing and increase confidence that missing data is not MNAR.

Finally, it may be worth looking into interactions between variables, which was not explored in this analysis. It is often true in medical setting that comorbidities interact with each other, so the predictive performance of the model may be aided by trying to capture these relationships.

# Conclusion

<!-- TO-DO -->

\newpage

# Appendix

## Tables and Figures

```{r echo = FALSE, warning = FALSE, message = FALSE}

library(tidyverse)
library(mice)
library(pROC)
library(rjags)
library(coda)
```

```{r echo = FALSE, warning = FALSE, message = FALSE}

load("strokeStudy.RData")
```

```{r echo = FALSE, warning = FALSE, message = FALSE}

x <- x |>
  mutate(Race2 = case_when(
    !is.na(Race2) & tolower(Race2) == "missing" ~ NA_character_,
    TRUE ~ as.character(Race2)
  ),
  across(all_of(c("Age","PreHospNotify","EMSvsCar","Race2")[c("Age","PreHospNotify","EMSvsCar","Race2") %in% names(x)]),
         as.character),
  Time2 = factor(Time2, levels = unique(Time2)))

missing_vars <- c("Age", "PreHospNotify", "EMSvsCar", "Race2")
missing_vars <- missing_vars[missing_vars %in% names(x)]
```

<!-- EDA AND DATA -->

```{r label = eda1, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Proportion of missingness by outcome for all variables with missingness", fig.show = 'hold'}

eda1 <- x |>
  filter(!is.na(homeOrRehab)) |>
  pivot_longer(all_of(missing_vars),
               names_to = "variable",
               values_to = "value") |>
  mutate(is_missing = is.na(value)) |>
  group_by(variable, homeOrRehab) |>
  summarize(prop_missing = mean(is_missing), .groups = "drop")

ggplot(eda1, aes(x = homeOrRehab, y = prop_missing, fill = homeOrRehab)) +
  geom_col(width = 0.65) +
  facet_wrap(~ variable) +
  scale_fill_manual(values = c("steelblue4", "skyblue3")) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Missingness by Outcome",
    x = "Outcome",
    y = "Percent Missing"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    strip.text = element_text(face = "bold")
  )
```

```{r label = eda2, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Heatmap of missingness by site and study quarter for all variables with missingness", fig.show = 'hold'}

eda2 <- x |>
  select(siteID, Time2, all_of(missing_vars)) |>
  pivot_longer(all_of(missing_vars),
               names_to = "variable",
               values_to = "value") |>
  mutate(is_missing = is.na(value)) |>
  group_by(siteID, Time2, variable) |>
  summarize(prop_missing = mean(is_missing), .groups = "drop")

ggplot(eda2, aes(x = Time2, y = siteID, fill = prop_missing)) +
  geom_tile() +
  facet_wrap(~ variable, nrow = 2) +
  scale_fill_distiller(
    palette = "Blues",
    direction = 1,
    labels = scales::percent_format()
  ) +
  labs(
    title = "Missingness by Site and Time",
    x = "Study Quarter",
    y = "Site",
    fill = "Percent Missing"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 30, hjust = 1, size = 8),
    strip.text = element_text(face = "bold")
  )
```

```{r echo = FALSE, warning = FALSE, message = FALSE}

strokes <- x

# factor conversions and TPA grouping
strokes <- strokes %>%
  mutate(
    EMSvsCar = factor(EMSvsCar),
    TPA_group = case_when(
      hadTPA == FALSE ~ "NoTPA",
      hadTPA == TRUE & tpaComplic == FALSE ~ "TPA_NoComp",
      hadTPA == TRUE & tpaComplic == TRUE ~ "TPA_Comp"
    ),
    TPA_group = factor(TPA_group, levels = c("NoTPA", "TPA_NoComp", "TPA_Comp")),
    siteID = factor(siteID)
  ) %>%
  select(-hadTPA, -tpaComplic)

data <- strokes

# STEP 1: Specify imputation methods
# "" means: do NOT impute the variable
method <- make.method(data)

# set intended imputation methods
method[c("Age", "EMSvsCar", "PreHospNotify")] <- c("pmm", "logreg", "logreg")

# ensure NO OTHER variables are imputed
other_vars <- setdiff(names(method), c("Age", "EMSvsCar", "PreHospNotify"))
method[other_vars] <- ""

# STEP 2: Build predictor matrix
pred <- make.predictorMatrix(data)

# Remove self-prediction for the imputed variables
pred[c("Age", "EMSvsCar", "PreHospNotify"), c("Age", "EMSvsCar", "PreHospNotify")] <-
  0 * pred[c("Age", "EMSvsCar", "PreHospNotify"), c("Age", "EMSvsCar", "PreHospNotify")]

# STEP 3: Set the order of imputation
visitSequence <- c("Age", "EMSvsCar", "PreHospNotify")

# STEP 4: Run MICE
imp <- mice(
  data,
  m = 20,
  method = method,
  predictorMatrix = pred,
  visitSequence = visitSequence,
  maxit = 20,
  print = FALSE
)
```

```{r echo = FALSE, warning = FALSE, message = FALSE}

completed_list <- mice::complete(imp, action = "all")

fix_rule <- function(df) {
  df$PreHospNotify[df$EMSvsCar == 0] <- "No"
  return(df)
}

completed_fixed <- lapply(completed_list, fix_rule)
```

```{r label = eda3, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Association between predictors and favorable discharge", fig.height = 6, fig.show = 'hold'}

# Use first imputed dataset
eda_df <- complete(imp, 1)

# Create age groups
eda_df <- eda_df %>%
  mutate(
    AgeGroup = case_when(
      Age < 55 ~ "<55",
      Age >= 55 & Age <= 64 ~ "55-64",
      Age >= 65 & Age <= 74 ~ "65-74",
      Age >= 75 ~ "75+",
      TRUE ~ NA_character_
    ),
    AgeGroup = factor(AgeGroup, levels = c("<55", "55-64", "65-74", "75+"))
  )

# Select variables and pivot
assoc_df <- eda_df %>%
  filter(!is.na(homeOrRehab)) %>%
  select(
    homeOrRehab,
    AgeGroup,
    Gender,
    Race2,
    EMSvsCar,
    PreHospNotify,
    hadThrombectomy,
    thrComplic,
    TPA_group
  ) %>%
  mutate(across(-homeOrRehab, as.character)) %>%  # Convert all predictors to character
  pivot_longer(
    cols = -homeOrRehab,
    names_to = "Variable",
    values_to = "Level"
  ) %>%
  filter(!is.na(Level)) %>%
  group_by(Variable, Level) %>%
  summarize(
    n = n(),
    prop_favorable = mean(homeOrRehab == TRUE),
    se = sqrt(prop_favorable * (1 - prop_favorable) / n),
    .groups = "drop"
  ) %>%
  mutate(
    Variable = recode(
      Variable,
      AgeGroup = "Age Group",
      Gender = "Gender",
      Race2 = "Race",
      EMSvsCar = "Arrival Mode",
      PreHospNotify = "Pre-Hospital Notification",
      hadThrombectomy = "Thrombectomy",
      thrComplic = "Thrombectomy Complication",
      TPA_group = "TPA Group"
    )
  )

# Plot
ggplot(assoc_df, aes(x = Level, y = prop_favorable)) +
  geom_col(fill = "skyblue3") +
  geom_errorbar(
    aes(
      ymin = pmax(0, prop_favorable - 1.96 * se),
      ymax = pmin(1, prop_favorable + 1.96 * se)
    ),
    width = 0.2
  ) +
  facet_wrap(~ Variable, scales = "free_x", nrow = 3) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
  labs(
    title = "Association Between Predictors and Favorable Discharge",
    x = "",
    y = "Percent Favorable Discharge"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    strip.text = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

<!-- FREQUENTIST -->

```{r echo = FALSE, warning = FALSE, message = FALSE}

results = NULL
AUCs = NULL
test <- readRDS("imputed_data")
for (i in 1:20){
  data <- test[[i]]
  data$homeOrRehab <- if_else(data$homeOrRehab,1,0)
  modelFit <- glm(factor(homeOrRehab) ~ ., data = data, family = "binomial")
  AUCs <- c(AUCs,auc(data$homeOrRehab,predict(modelFit,data)))
  results <- c(results, list(summary(modelFit)))
}
```

```{r label = freq1, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Frequentist binned residual plot", fig.show = 'hold'}

arm::binnedplot(fitted(modelFit),residuals(modelFit),main="Binned residual plot for frequentist fit")
```

```{r label = freq2, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Frequentist ROC curve", fig.show = 'hold', eval = FALSE}

# [!] not talked about in writeup, so currently has eval = FALSE

plot.roc(data$homeOrRehab,predict(modelFit,data),main="ROC curve for frequentist fit")
```

\newpage

```{r label = freq3, echo = FALSE, warning = FALSE, message = FALSE}

results[[1]]$coefficients |> 
  as.data.frame() |> 
  select(-`z value`) |> 
  kableExtra::kable(caption = "Coefficient estimates for frequentist fit")
```

```{r label = freq4, echo = FALSE, warning = FALSE, message = FALSE}

data.frame(iter = seq(1,20), AUC = AUCs) |> 
  kableExtra::kable(caption = "AUC values for all 20 frequentist fits")
```

\newpage

<!-- BAYESIAN -->

```{r echo = FALSE, warning = FALSE, message = FALSE}

# ----------------------------
# 1. Data Wrangling & Baselines
# ----------------------------
strokes$Age[strokes$Age == ""] <- NA
strokes$EMSvsCar[strokes$EMSvsCar == ""] <- NA
strokes$PreHospNotify[strokes$PreHospNotify == ""] <- NA
strokes$Race2[strokes$Race2 == "Missing"] <- NA

# Race baseline = "Other"
strokes$Race2 <- factor(strokes$Race2, levels = c("Caucasian", "African American", "Other"))
strokes$Race2 <- relevel(strokes$Race2, ref = "Other")

# TransportNotify baseline = "Other"
strokes$TransportNotify <- factor(case_when(
  strokes$EMSvsCar == "1" & strokes$PreHospNotify == "Yes" ~ "Yes",
  strokes$EMSvsCar == "1" & strokes$PreHospNotify == "No" ~ "No",
  TRUE ~ "Other"
), levels = c("Yes", "No", "Other"))
strokes$TransportNotify <- relevel(strokes$TransportNotify, ref = "Other")

# Time2 baseline = "1"
strokes$Time2 <- factor(case_when(
  strokes$Time2 %in% c("Y1Q1", "Y1Q2") ~ "1",
  strokes$Time2 %in% c("Y2Q3", "Y2Q4") ~ "3",
  TRUE ~ "2"
), levels = c("1","2","3"))
strokes$Time2 <- relevel(strokes$Time2, ref = "1")

# Other covariates
strokes$homeOrRehab <- as.integer(strokes$homeOrRehab)
strokes$siteID <- as.numeric(factor(strokes$siteID))

# Drop unnecessary vars
strokes <- strokes %>% select(-PreHospNotify, -EMSvsCar)

# ----------------------------
# 1. Prepare data
# ----------------------------

# Gender baseline = "Male"
strokes$Gender <- factor(strokes$Gender, levels = c("Male","Female"))
strokes$Gender <- relevel(strokes$Gender, ref = "Male")

# Race, TransportNotify, Time2 as factors with explicit baselines
strokes$Race2 <- factor(strokes$Race2, levels = c("Caucasian","African American","Other"))
strokes$Race2 <- relevel(strokes$Race2, ref = "Caucasian")

strokes$TransportNotify <- factor(strokes$TransportNotify, levels = c("Yes","No","Other"))
strokes$TransportNotify <- relevel(strokes$TransportNotify, ref = "Yes")

strokes$Time2 <- factor(strokes$Time2, levels = c("1","2","3"))
strokes$Time2 <- relevel(strokes$Time2, ref = "1")

# ----------------------------
# 2. Build covariate matrix X
# ----------------------------
exclude_vars <- c("Age","Race2","TransportNotify","Time2","siteID","homeOrRehab")
covariate_names <- setdiff(names(strokes), exclude_vars)

# Ensure Gender is included
if(!"Gender" %in% covariate_names) covariate_names <- c(covariate_names, "Gender")

# Use treatment contrasts, keep intercept for baseline
X_df <- model.matrix(~ ., data = strokes[, covariate_names, drop=FALSE],
                     contrasts.arg = lapply(strokes[, covariate_names, drop=FALSE],
                                            function(x) if(is.factor(x)) "contr.treatment" else NULL))
X_df[is.na(X_df)] <- 0
X <- as.matrix(X_df)
NCOV <- ncol(X)
cov_names <- colnames(X)

# ----------------------------
# 3. Categorical indices for JAGS
# ----------------------------
race_idx <- as.numeric(strokes$Race2)
tr_idx <- as.numeric(strokes$TransportNotify)
time_idx <- as.numeric(strokes$Time2)
site_idx <- strokes$siteID

N <- nrow(strokes)
nsite <- length(unique(site_idx))
K_race <- length(levels(strokes$Race2))
K_tr <- length(levels(strokes$TransportNotify))
K_time <- length(levels(strokes$Time2))

# ----------------------------
# 4. JAGS data
# ----------------------------
jags_data <- list(
  N = N,
  X = X,
  NCOV = NCOV,
  site = site_idx,
  nsite = nsite,
  time = time_idx,
  K_time = K_time,
  Y = strokes$homeOrRehab,
  race = race_idx,
  tr = tr_idx,
  K_race = K_race,
  K_tr = K_tr,
  age = as.numeric(strokes$Age)
)

# ----------------------------
# 5. JAGS model string
# ----------------------------
model_string <- "
model {
  # Age imputation
  for(i in 1:N){
    age[i] ~ dnorm(mu_age[i], tau_age)
    mu_age[i] <- beta0_age + b_site_age[site[i]]
  }
  beta0_age ~ dnorm(0,0.001)
  tau_age ~ dgamma(0.01,0.01)
  for(s in 1:nsite){ b_site_age[s] ~ dnorm(0, tau_b_age) }
  tau_b_age ~ dgamma(0.01,0.01)

  # Race imputation
  pi_race[1:K_race] ~ ddirch(rep(1,K_race))
  for(i in 1:N){ race[i] ~ dcat(pi_race[1:K_race]) }
  beta_race[1] <- 0
  for(k in 2:K_race){ beta_race[k] ~ dnorm(0,0.001) }

  # TransportNotify imputation
  pi_tr[1:K_tr] ~ ddirch(rep(1,K_tr))
  for(i in 1:N){ tr[i] ~ dcat(pi_tr[1:K_tr]) }
  beta_tr[1] <- 0
  for(k in 2:K_tr){ beta_tr[k] ~ dnorm(0,0.001) }

  # Time effects
  beta_time[1] <- 0
  for(k in 2:K_time){ beta_time[k] ~ dnorm(0,0.001) }

  # Logistic regression
  for(i in 1:N){
    Y[i] ~ dbern(p[i])
    logit(p[i]) <- beta0 + inprod(beta[], X[i,]) +
                   beta_age*age[i] +
                   beta_race[race[i]] +
                   beta_tr[tr[i]] +
                   beta_time[time[i]] +
                   b_site_home[site[i]]
  }

  # Priors
  beta0 ~ dnorm(0,0.001)
  for(j in 1:NCOV){ beta[j] ~ dnorm(0,0.001) }
  beta_age ~ dnorm(0,0.001)
  for(s in 1:nsite){ b_site_home[s] ~ dnorm(0, tau_home) }
  tau_home ~ dgamma(0.01,0.01)
}
"
writeLines(model_string, "model_impute_all_fixed_gender.txt")

# ----------------------------
# 6. Initial values
# ----------------------------
inits_fn <- function(){
  inits <- list()
  inits$beta0 <- 0
  inits$beta <- rep(0, NCOV)
  inits$beta_age <- 0
  inits$b_site_home <- rep(0, nsite)
  inits$b_site_age <- rep(0, nsite)
  inits$tau_home <- 1
  inits$tau_age <- 1
  inits$tau_b_age <- 1

  # Age missing
  age_init <- rep(NA_real_, N)
  missing_age <- which(is.na(jags_data$age))
  if(length(missing_age) > 0) age_init[missing_age] <- mean(strokes$Age, na.rm = TRUE)
  inits$age <- age_init

  # Race missing
  race_init <- rep(NA_integer_, N)
  missing_race <- which(is.na(jags_data$race))
  if(length(missing_race) > 0) race_init[missing_race] <- sample(1:K_race, length(missing_race), replace = TRUE)
  inits$race <- race_init

  # TransportNotify missing
  tr_init <- rep(NA_integer_, N)
  missing_tr <- which(is.na(jags_data$tr))
  if(length(missing_tr) > 0) tr_init[missing_tr] <- sample(1:K_tr, length(missing_tr), replace = TRUE)
  inits$tr <- tr_init

  # Dirichlet
  inits$pi_race <- rep(1/K_race, K_race)
  inits$pi_tr <- rep(1/K_tr, K_tr)
  return(inits)
}
inits_list <- list(inits_fn(), inits_fn(), inits_fn())

# ----------------------------
# 7. Run JAGS
# ----------------------------
jmod <- jags.model(file = "model_impute_all_fixed_gender.txt",
                   data = jags_data,
                   inits = inits_list,
                   n.chains = 3,
                   n.adapt = 1000)
update(jmod, 2000)

monitor_vars <- c("beta0","beta","beta_age","beta_race","beta_tr","beta_time","b_site_home")
samples <- coda.samples(jmod, variable.names = monitor_vars, n.iter = 20000)

# ----------------------------
# 8. Extract coefficients and credible intervals
# ----------------------------
samples_matrix <- as.matrix(samples)

# Base names
coef_names <- c("Intercept", colnames(X), "Age")

# Add categorical non-baseline levels
factor_list <- list(
  Race2 = levels(strokes$Race2),
  TransportNotify = levels(strokes$TransportNotify),
  Time2 = levels(strokes$Time2)
)
for(f in names(factor_list)){
  levs <- factor_list[[f]]
  if(length(levs) > 1){
    for(k in 2:length(levs)){
      coef_names <- c(coef_names, paste0(f, "_", levs[k]))
    }
  }
}

# Map JAGS columns
param_order <- c("beta0", paste0("beta[", 1:NCOV, "]"), "beta_age")
if(K_race > 1) param_order <- c(param_order, paste0("beta_race[", 2:K_race, "]"))
if(K_tr > 1) param_order <- c(param_order, paste0("beta_tr[", 2:K_tr, "]"))
if(K_time > 1) param_order <- c(param_order, paste0("beta_time[", 2:K_time, "]"))

coef_samples <- samples_matrix[, param_order, drop=FALSE]
```

```{r label = bayes1, echo = FALSE, warning = FALSE, message = FALSE}

summary_df <- data.frame(
  Variable = coef_names,
  Median = apply(coef_samples, 2, median),
  Lower95 = apply(coef_samples, 2, quantile, probs=0.025),
  Upper95 = apply(coef_samples, 2, quantile, probs=0.975)
) %>%
  mutate(across(2:4, round, 3)) %>%
  kableExtra::kable(caption = "Posterior Summary Statistics (Median and 95% Credible Intervals)")

summary_df
```

```{r label = bayes2, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Posterior Odds Ratio with 95% Credible Interval", fig.show = 'hold'}

# compute posterior median and 95% CrI on log-odds scale
median_log  <- apply(coef_samples, 2, median, na.rm = TRUE)
lower95_log <- apply(coef_samples, 2, quantile, probs = 0.025, na.rm = TRUE)
upper95_log <- apply(coef_samples, 2, quantile, probs = 0.975, na.rm = TRUE)

# table (param_order and coef_names are aligned by construction above)
or_tab <- data.frame(
  jags_param = param_order,
  label = coef_names,
  median_log = as.numeric(median_log[param_order]),
  lower95_log = as.numeric(lower95_log[param_order]),
  upper95_log = as.numeric(upper95_log[param_order]),
  stringsAsFactors = FALSE
)

# drop intercept (beta0) so only variables shown
or_tab <- or_tab %>% filter(!(jags_param %in% c("beta0") | label %in% c("Intercept", "(Intercept)")))

# convert to OR scale (median of log -> exp(median); and quantiles -> exp(quantiles))
or_tab <- or_tab %>%
  mutate(
    OR = exp(median_log),
    OR_low = exp(lower95_log),
    OR_high = exp(upper95_log)
  )

# order for plotting (adjust to taste)
or_tab <- or_tab %>%
  arrange(OR) %>%
  mutate(label = factor(label, levels = label))   # keeps arranged order (bottom -> top)

# plot
p_or_med <- ggplot(or_tab, aes(x = OR, y = label)) +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = OR_low, xmax = OR_high), height = 0.25) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "grey40") +
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.1))) +
  labs(
    title = "Posterior Odds Ratios (Median, 95% CrI)",
    x = "Odds Ratio (Median) with 95% CrI",
    y = ""
  ) +
  theme_minimal(base_size = 13) +
  theme(panel.grid.minor = element_blank(), axis.text.y = element_text(size = 11))

print(p_or_med)
```





