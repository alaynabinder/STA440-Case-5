---
title: "Analysis"
output: pdf_document
---

```{r}
library(tidyverse)
```

```{r}
load("strokeStudy.RData")
```


```{r}
x$siteID <- as.character(x$siteID)
unique(x$siteID)

```


# EDA (MISSINGNESS)

```{r}
#| label: missingness-by-outcome

# Remove NAs from outcome
x <- x |>
  filter(!is.na(homeOrRehab))

missing_vars <- c("Age", "PreHospNotify", "EMSvsCar")

x_missing_outcome <- x |>
  mutate(
    homeOrRehab = factor(homeOrRehab),
    across(all_of(missing_vars), as.character)
  ) |>
  pivot_longer(
    cols = all_of(missing_vars),
    names_to = "variable",
    values_to = "value"
  ) |>
  mutate(is_missing = is.na(value)) |>
  group_by(variable, homeOrRehab) |>
  summarize(prop_missing = mean(is_missing), .groups = "drop")

ggplot(x_missing_outcome,
       aes(x = homeOrRehab, y = prop_missing, fill = homeOrRehab)) +
  geom_col() +
  facet_wrap(~ variable) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Missingness by Outcome",
    x = "Outcome",
    y = "Percent Missing"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
#| label: missingness-by-site-and-time

missing_by_site_time <- x |>
  select(siteID, Time2, all_of(missing_vars)) |>
  mutate(across(all_of(missing_vars), as.character)) |>
  pivot_longer(
    cols = all_of(missing_vars),
    names_to = "variable",
    values_to = "value"
  ) |>
  mutate(is_missing = is.na(value)) |>
  group_by(siteID, Time2, variable) |>
  summarize(prop_missing = mean(is_missing), .groups = "drop")

ggplot(missing_by_site_time,
       aes(x = Time2, y = siteID, fill = prop_missing)) +
  geom_tile() +
  facet_wrap(~ variable, nrow = 2) +
  scale_fill_continuous(labels = scales::percent_format()) +
  labs(
    title = "Missingness by Site and Time",
    x = "Study Quarter",
    y = "Site",
    fill = "Percent Missing"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 30, hjust = 1, size = 8),
    strip.text = element_text(size = 12)
  )

library(mice)
md.pattern(x,rotate.names = TRUE)
```

# DATA CLEANING / IMPUTATION

## Packages

```{r}
library(dplyr)
```

## Data Cleaning


```{r}
strokes <- x
strokes$EMSvsCar <- factor(strokes$EMSvsCar)

```

```{r}
strokes |> 
  group_by(hadTPA, tpaComplic) |>
  count()

```


```{r}

strokes <- strokes %>%
  mutate(TPA_group = case_when(
    hadTPA == FALSE ~ "NoTPA",
    hadTPA == TRUE & tpaComplic == FALSE ~ "TPA_NoComp",
    hadTPA == TRUE & tpaComplic == TRUE  ~ "TPA_Comp"
  ))

# Optional: make it a factor with ordered levels
strokes <- strokes %>%
  mutate(TPA_group = factor(TPA_group, levels = c("NoTPA", "TPA_NoComp", "TPA_Comp")))

# Check result
table(strokes$TPA_group)

strokes <- strokes |>
  select(-hadTPA) |>
  select(-tpaComplic)

```
There is some correlation with hadTP and tpaComplication, so we made a new variable and removed the other two.


## Missingness

### First impute age, then EMS vs Car,  then Notify

I chose this order because of the dependency thing (unless I misunderstood that). 


PreHospNotify cannot be imputed without knowing EMSvsCar first. --> this is because when EMSvsCar = 1, PreHospNotify has be No.

See
```{r}
strokes |> group_by(strokes$EMSvsCar, PreHospNotify) |> count()
```


We need to maintain the constraint thought that if it's car, there won't be notify. --> I asked and because there's not a forced rule for when EMS vs Car equals 1, i.e. EMS could be PreHospNotify = YES or NO when it's 1. It recommended resolving using post-processing. 



```{r}
library(mice)

# Select relevant variables or full dataset
#data <- strokes |>
  #select(-siteID)


strokes$siteID <-factor(strokes$siteID)
data <- strokes

# STEP 1: Specify imputation methods
# "" means: do NOT impute the variable
method <- make.method(data)

method["Age"] <- "pmm"
method["EMSvsCar"] <- "logreg"         # assuming it's binary
method["PreHospNotify"] <- "logreg"    # assuming it's binary

# Make sure NO OTHER variables are imputed
for (v in names(method)) {
  if (!v %in% c("Age", "EMSvsCar", "PreHospNotify")) {
    method[v] <- ""
  }
}

# STEP 2: Build predictor matrix
pred <- make.predictorMatrix(data)

# Remove self-prediction
pred["Age", "Age"] <- 0
pred["EMSvsCar", "EMSvsCar"] <- 0
pred["PreHospNotify", "PreHospNotify"] <- 0

# (OPTIONAL) If you want all variables to predict the three variables,
# keep predictor matrix as is.
# Or customize:
# pred["EMSvsCar", ] <- 0
# pred["EMSvsCar", c("Age")] <- 1

# STEP 3: Set the order of imputation
visitSequence <- c("Age", "EMSvsCar", "PreHospNotify")

# STEP 4: Run MICE
imp <- mice(
  data,
  m = 20,
  method = method,
  predictorMatrix = pred,
  visitSequence = visitSequence,
  maxit = 20,
  print = FALSE
)

```
### my post-processing

holds that all cases where EMS vs Car = 0, there won't be a prehospital notify  

```{r}
completed_list <- mice::complete(imp, action = "all")

# Define the categories exactly as they appear in your data
fix_rule <- function(df) {
  df$PreHospNotify[df$EMSvsCar == 0] <- "No"
  return(df)
}

completed_fixed <- lapply(completed_list, fix_rule)


```


### Rule out MNAR

Need to validate we can make the MAR assumption. These are diagnostics. 



```{r eval=FALSE}
flip_some <- function(x, prop = 0.1) {
  # x: factor with 2 levels
  levels_x <- levels(x)
  if(length(levels_x) < 2) return(x)  # cannot flip
  idx <- sample(which(x == levels_x[1]), size = floor(prop*sum(x==levels_x[1])))
  x[idx] <- levels_x[2]
  return(x)
}

delta_age <- c(-1, 0, 1)  # small numeric shifts
delta_ems <- c(-1, 0, 1)  # only flip some cases
delta_pre <- c("flip_some", "none")

sensitivity_results <- list()

for(a in delta_age){
  for(e in delta_ems){
    for(pn in delta_pre){
      
      adjusted_dfs <- lapply(completed_fixed, function(df){
        df_new <- df
        
        # AGE adjustment
        df_new$Age <- df_new$Age + a
        
        # EMSvsCar adjustment (flip some)
        if(e != 0){
          df_new$EMSvsCar <- flip_some(df_new$EMSvsCar, prop = 0.1)
        }
        
        # PreHospNotify adjustment (flip some)
        if(pn == "flip_some"){
          df_new$PreHospNotify <- flip_some(df_new$PreHospNotify, prop = 0.1)
        }
        
        df_new
      })
      
      # Fit models and pool
      model_list <- lapply(adjusted_dfs, function(df){
        glm(homeOrRehab ~ Age + EMSvsCar + PreHospNotify,
            data = df, family = binomial)
      })
      
      pooled <- pool(model_list)
      pooled_summary <- summary(pooled)
      
      scenario_name <- paste0("Age_", a, "_EMS_", e, "_Pre_", pn)
      sensitivity_results[[scenario_name]] <- pooled_summary
    }
  }
}

```


```{r,eval=FALSE}
model_results <- lapply(sensitivity_age, function(s) {
  with(mice::as.mids(s$data), glm(StrokeOutcome ~ Age + EMSvsCar + PreHospNotify, family=binomial))
})

library(dplyr)

# Extract key variables (Age, EMSvsCar, PreHospNotify)
extract_var <- function(var_name){
  lapply(names(sensitivity_results), function(s){
    res <- sensitivity_results[[s]]
    
    # Skip if not a data frame
    if(!is.data.frame(res)) return(NULL)
    
    # Filter variable row
    df <- res %>%
      filter(term == var_name) %>%
      select(term, estimate, std.error, p.value) %>%
      mutate(scenario = s)
    
    df
  }) %>% bind_rows()
}

# Get estimates for each variable
age_results <- extract_var("Age")
ems_results <- extract_var("EMSvsCar")
pre_results <- extract_var("PreHospNotify")

```

```{r}
library(ggplot2)

# Combine all variables into one data frame for plotting

if (FALSE)
{
sensitivity_plot_data <- bind_rows(
  age_results %>% mutate(variable = "Age"),
  ems_results %>% mutate(variable = "EMSvsCar"),
  pre_results %>% mutate(variable = "PreHospNotify")
)

# Optional: create a simpler scenario label
sensitivity_plot_data <- sensitivity_plot_data %>%
  mutate(scenario_short = gsub("Age_|_EMS_|_Pre_", "", scenario))

# Plot
ggplot(sensitivity_plot_data, aes(x = scenario, y = estimate, color = variable)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = estimate - 1.96*std.error, ymax = estimate + 1.96*std.error),
                width = 0.2, position = position_dodge(width = 0.5)) +
  facet_wrap(~ variable, scales = "free_y") +
  labs(
    title = "Sensitivity Analysis: Effect Estimates Across Scenarios",
    x = "Scenario",
    y = "Estimated Coefficient (log-odds)",
    color = "Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

```



# EDA (POST IMPUTATION)

First is just checking the distributions pre and post imputation (they should be approximately the same):

```{r}
# Combining all completed datasets into a long dataframe
post_imp_long <- bind_rows(
  lapply(seq_along(completed_fixed), function(i) {
    completed_fixed[[i]] |> mutate(.imp = i)
  })
)

# Age
orig <- x |> mutate(.imp = "Original")

post_imp_long2 <- post_imp_long |> 
  mutate(.imp = as.character(.imp))

age_compare <- bind_rows(
  orig |> select(Age, .imp),
  post_imp_long2 |> select(Age, .imp)
)

ggplot(age_compare, aes(x = Age, fill = factor(.imp))) +
  geom_density(alpha = 0.2) +
  labs(
    title = "Age Distribution: Original vs. Imputed Datasets",
    x = "Age",
    y = "Density",
    fill = "Dataset"
  ) +
  theme_minimal() +
  facet_wrap(~factor(.imp))
```

```{r,eval=FALSE}
# EMSvsCar, PreHospNotify
orig_summary <- x |>
  summarize(
    EMSvsCar = mean(EMSvsCar == 1, na.rm = TRUE),
    PreHospNotify = mean(PreHospNotify == "Yes", na.rm = TRUE)
  ) |>
  mutate(.imp = "Original") |>
  pivot_longer(
    cols = c("EMSvsCar", "PreHospNotify"),
    names_to = "variable",
    values_to = "prop_yes"
  )

imp_summary <- post_imp_long |>
  mutate(
    .imp = as.character(.imp),
    EMSvsCar = as.numeric(as.character(EMSvsCar)),
    PreHospNotify = as.character(PreHospNotify)
  ) |>
  group_by(.imp) |>
  summarize(
    EMSvsCar = mean(EMSvsCar == 1, na.rm = TRUE),
    PreHospNotify = mean(PreHospNotify == "Yes", na.rm = TRUE)
  ) |>
  pivot_longer(
    cols = c("EMSvsCar", "PreHospNotify"),
    names_to = "variable",
    values_to = "prop_yes"
  )

binary_summary_all <- binary_summary_all |>
  mutate(
    .imp = factor(.imp, levels = c("Original", as.character(1:20)))
  )

ggplot(binary_summary_all, aes(x = .imp, y = prop_yes, fill = (.imp == "Original"))) +
  geom_col() +
  facet_wrap(~ variable, nrow = 1) +
  scale_fill_manual(values = c("grey70", "tomato"), guide = "none") +
  labs(
    title = "Proportion of Binary Variables: Original vs. Imputed Datasets",
    x = "Dataset",
    y = "Proportion 'Yes'"
  ) +
  coord_cartesian(ylim = c(0, 1)) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

Now onto EDA for association:

```{r, fig.width=18, fig.height=10,eval=FALSE}
# just doing eda with 1st bc distributions roughly the same - obviously we would want to use all for prediction, but more lax in this case
assoc_df <- eda_df |>
  select(
    Disposition,
    AgeGroup,
    Gender,
    Race2,
    ArrivalMode,
    PreHospitalNotification,
    Thrombectomy,
    TPAComplication,
    ThrombectomyComplication,
    StudyQuarter
  ) |>
  pivot_longer(
    cols = -Disposition,
    names_to = "Variable",
    values_to = "Level"
  ) |>
  group_by(Variable, Level) |>
  summarize(
    n = n(),
    prop_favorable = mean(Disposition == "Favorable Discharge"),
    se = sqrt(prop_favorable * (1 - prop_favorable) / n),
    .groups = "drop"
  ) |>
  mutate(
    Variable = recode(
      Variable,
      AgeGroup = "Age Group",
      Gender = "Gender",
      Race2 = "Race",
      ArrivalMode = "Arrival Mode",
      PreHospitalNotification = "Pre-Hospital Notification",
      Thrombectomy = "Thrombectomy",
      TPAComplication = "TPA Complication",
      ThrombectomyComplication = "Thrombectomy Complication",
      StudyQuarter = "Study Quarter"
    )
  )

ggplot(assoc_df, aes(x = Level, y = prop_favorable)) +
  geom_col(fill = "#4472C4") +
  geom_errorbar(
    aes(
      ymin = prop_favorable - 1.96 * se,
      ymax = prop_favorable + 1.96 * se
    ),
    width = 0.2
  ) +
  facet_wrap(~ Variable, scales = "free_x", nrow = 2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
  labs(
    title = "Association Between Predictors and Favorable Discharge",
    x = "",
    y = "Percent Favorable Discharge"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    strip.text = element_text(face = "bold", size = 16),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    plot.title = element_text(size = 20, face = "bold")
  )
```


# IMPUTATION + BAYESIAN COMBINED MODEL

```{r}
library(dplyr)
library(rjags)
library(coda)

# ----------------------------
# 1. Data Wrangling & Baselines
# ----------------------------
strokes$Age[strokes$Age == ""] <- NA
strokes$EMSvsCar[strokes$EMSvsCar == ""] <- NA
strokes$PreHospNotify[strokes$PreHospNotify == ""] <- NA
strokes$Race2[strokes$Race2 == "Missing"] <- NA

# Race baseline = "Other"
strokes$Race2 <- factor(strokes$Race2, levels = c("Caucasian", "African American", "Other"))
strokes$Race2 <- relevel(strokes$Race2, ref = "Other")

# TransportNotify baseline = "Other"
strokes$TransportNotify <- factor(case_when(
  strokes$EMSvsCar == "1" & strokes$PreHospNotify == "Yes" ~ "Yes",
  strokes$EMSvsCar == "1" & strokes$PreHospNotify == "No" ~ "No",
  TRUE ~ "Other"
), levels = c("Yes", "No", "Other"))
strokes$TransportNotify <- relevel(strokes$TransportNotify, ref = "Other")

# Time2 baseline = "1"
strokes$Time2 <- factor(case_when(
  strokes$Time2 %in% c("Y1Q1", "Y1Q2") ~ "1",
  strokes$Time2 %in% c("Y2Q3", "Y2Q4") ~ "3",
  TRUE ~ "2"
), levels = c("1","2","3"))
strokes$Time2 <- relevel(strokes$Time2, ref = "1")

# Other covariates
strokes$homeOrRehab <- as.integer(strokes$homeOrRehab)
strokes$siteID <- as.numeric(factor(strokes$siteID))

# Drop unnecessary vars
strokes <- strokes %>% select(-PreHospNotify, -EMSvsCar)

# ----------------------------
# 1. Prepare data
# ----------------------------

# Gender baseline = "Male"
strokes$Gender <- factor(strokes$Gender, levels = c("Male","Female"))
strokes$Gender <- relevel(strokes$Gender, ref = "Male")

# Race, TransportNotify, Time2 as factors with explicit baselines
strokes$Race2 <- factor(strokes$Race2, levels = c("Caucasian","African American","Other"))
strokes$Race2 <- relevel(strokes$Race2, ref = "Caucasian")

strokes$TransportNotify <- factor(strokes$TransportNotify, levels = c("Yes","No","Other"))
strokes$TransportNotify <- relevel(strokes$TransportNotify, ref = "Yes")

strokes$Time2 <- factor(strokes$Time2, levels = c("1","2","3"))
strokes$Time2 <- relevel(strokes$Time2, ref = "1")

# ----------------------------
# 2. Build covariate matrix X
# ----------------------------
exclude_vars <- c("Age","Race2","TransportNotify","Time2","siteID","homeOrRehab")
covariate_names <- setdiff(names(strokes), exclude_vars)

# Ensure Gender is included
if(!"Gender" %in% covariate_names) covariate_names <- c(covariate_names, "Gender")

# Use treatment contrasts, keep intercept for baseline
X_df <- model.matrix(~ ., data = strokes[, covariate_names, drop=FALSE],
                     contrasts.arg = lapply(strokes[, covariate_names, drop=FALSE],
                                            function(x) if(is.factor(x)) "contr.treatment" else NULL))
X_df[is.na(X_df)] <- 0
X <- as.matrix(X_df)
NCOV <- ncol(X)
cov_names <- colnames(X)

# ----------------------------
# 3. Categorical indices for JAGS
# ----------------------------
race_idx <- as.numeric(strokes$Race2)
tr_idx <- as.numeric(strokes$TransportNotify)
time_idx <- as.numeric(strokes$Time2)
site_idx <- strokes$siteID

N <- nrow(strokes)
nsite <- length(unique(site_idx))
K_race <- length(levels(strokes$Race2))
K_tr <- length(levels(strokes$TransportNotify))
K_time <- length(levels(strokes$Time2))

# ----------------------------
# 4. JAGS data
# ----------------------------
jags_data <- list(
  N = N,
  X = X,
  NCOV = NCOV,
  site = site_idx,
  nsite = nsite,
  time = time_idx,
  K_time = K_time,
  Y = strokes$homeOrRehab,
  race = race_idx,
  tr = tr_idx,
  K_race = K_race,
  K_tr = K_tr,
  age = as.numeric(strokes$Age)
)

# ----------------------------
# 5. JAGS model string
# ----------------------------
model_string <- "
model {
  # Age imputation
  for(i in 1:N){
    age[i] ~ dnorm(mu_age[i], tau_age)
    mu_age[i] <- beta0_age + b_site_age[site[i]]
  }
  beta0_age ~ dnorm(0,0.001)
  tau_age ~ dgamma(0.01,0.01)
  for(s in 1:nsite){ b_site_age[s] ~ dnorm(0, tau_b_age) }
  tau_b_age ~ dgamma(0.01,0.01)

  # Race imputation
  pi_race[1:K_race] ~ ddirch(rep(1,K_race))
  for(i in 1:N){ race[i] ~ dcat(pi_race[1:K_race]) }
  beta_race[1] <- 0
  for(k in 2:K_race){ beta_race[k] ~ dnorm(0,0.001) }

  # TransportNotify imputation
  pi_tr[1:K_tr] ~ ddirch(rep(1,K_tr))
  for(i in 1:N){ tr[i] ~ dcat(pi_tr[1:K_tr]) }
  beta_tr[1] <- 0
  for(k in 2:K_tr){ beta_tr[k] ~ dnorm(0,0.001) }

  # Time effects
  beta_time[1] <- 0
  for(k in 2:K_time){ beta_time[k] ~ dnorm(0,0.001) }

  # Logistic regression
  for(i in 1:N){
    Y[i] ~ dbern(p[i])
    logit(p[i]) <- beta0 + inprod(beta[], X[i,]) +
                   beta_age*age[i] +
                   beta_race[race[i]] +
                   beta_tr[tr[i]] +
                   beta_time[time[i]] +
                   b_site_home[site[i]]
  }

  # Priors
  beta0 ~ dnorm(0,0.001)
  for(j in 1:NCOV){ beta[j] ~ dnorm(0,0.001) }
  beta_age ~ dnorm(0,0.001)
  for(s in 1:nsite){ b_site_home[s] ~ dnorm(0, tau_home) }
  tau_home ~ dgamma(0.01,0.01)
}
"
writeLines(model_string, "model_impute_all_fixed_gender.txt")

# ----------------------------
# 6. Initial values
# ----------------------------
inits_fn <- function(){
  inits <- list()
  inits$beta0 <- 0
  inits$beta <- rep(0, NCOV)
  inits$beta_age <- 0
  inits$b_site_home <- rep(0, nsite)
  inits$b_site_age <- rep(0, nsite)
  inits$tau_home <- 1
  inits$tau_age <- 1
  inits$tau_b_age <- 1

  # Age missing
  age_init <- rep(NA_real_, N)
  missing_age <- which(is.na(jags_data$age))
  if(length(missing_age) > 0) age_init[missing_age] <- mean(strokes$Age, na.rm = TRUE)
  inits$age <- age_init

  # Race missing
  race_init <- rep(NA_integer_, N)
  missing_race <- which(is.na(jags_data$race))
  if(length(missing_race) > 0) race_init[missing_race] <- sample(1:K_race, length(missing_race), replace = TRUE)
  inits$race <- race_init

  # TransportNotify missing
  tr_init <- rep(NA_integer_, N)
  missing_tr <- which(is.na(jags_data$tr))
  if(length(missing_tr) > 0) tr_init[missing_tr] <- sample(1:K_tr, length(missing_tr), replace = TRUE)
  inits$tr <- tr_init

  # Dirichlet
  inits$pi_race <- rep(1/K_race, K_race)
  inits$pi_tr <- rep(1/K_tr, K_tr)
  return(inits)
}
inits_list <- list(inits_fn(), inits_fn(), inits_fn())

# ----------------------------
# 7. Run JAGS
# ----------------------------
jmod <- jags.model(file = "model_impute_all_fixed_gender.txt",
                   data = jags_data,
                   inits = inits_list,
                   n.chains = 3,
                   n.adapt = 1000)
update(jmod, 2000)

monitor_vars <- c("beta0","beta","beta_age","beta_race","beta_tr","beta_time","b_site_home")
samples <- coda.samples(jmod, variable.names = monitor_vars, n.iter = 20000)

# ----------------------------
# 8. Extract coefficients and credible intervals
# ----------------------------
samples_matrix <- as.matrix(samples)

# Base names
coef_names <- c("Intercept", colnames(X), "Age")

# Add categorical non-baseline levels
factor_list <- list(
  Race2 = levels(strokes$Race2),
  TransportNotify = levels(strokes$TransportNotify),
  Time2 = levels(strokes$Time2)
)
for(f in names(factor_list)){
  levs <- factor_list[[f]]
  if(length(levs) > 1){
    for(k in 2:length(levs)){
      coef_names <- c(coef_names, paste0(f, "_", levs[k]))
    }
  }
}

# Map JAGS columns
param_order <- c("beta0", paste0("beta[", 1:NCOV, "]"), "beta_age")
if(K_race > 1) param_order <- c(param_order, paste0("beta_race[", 2:K_race, "]"))
if(K_tr > 1) param_order <- c(param_order, paste0("beta_tr[", 2:K_tr, "]"))
if(K_time > 1) param_order <- c(param_order, paste0("beta_time[", 2:K_time, "]"))

coef_samples <- samples_matrix[, param_order, drop=FALSE]

# Posterior summaries
summary_df <- data.frame(
  Variable = coef_names,
  Mean = apply(coef_samples, 2, mean),
  Lower95 = apply(coef_samples, 2, quantile, probs=0.025),
  Upper95 = apply(coef_samples, 2, quantile, probs=0.975)
) %>%
  mutate(across(2:4, round, 3))

print(summary_df)

```

```{r}
# ---- Posterior Odds Ratio forest (median + 95% CrI) ----
library(dplyr)
library(ggplot2)

# safety checks
stopifnot(exists("coef_samples"), exists("param_order"), exists("coef_names"))

# compute posterior median and 95% CrI on log-odds scale
median_log  <- apply(coef_samples, 2, median, na.rm = TRUE)
lower95_log <- apply(coef_samples, 2, quantile, probs = 0.025, na.rm = TRUE)
upper95_log <- apply(coef_samples, 2, quantile, probs = 0.975, na.rm = TRUE)

# assemble table (param_order and coef_names are aligned by construction above)
or_tab <- data.frame(
  jags_param = param_order,
  label = coef_names,
  median_log = as.numeric(median_log[param_order]),
  lower95_log = as.numeric(lower95_log[param_order]),
  upper95_log = as.numeric(upper95_log[param_order]),
  stringsAsFactors = FALSE
)

# drop intercept (beta0) so only variables shown
or_tab <- or_tab %>% filter(!(jags_param %in% c("beta0") | label %in% c("Intercept", "(Intercept)")))

# convert to OR scale (median of log -> exp(median); and quantiles -> exp(quantiles))
or_tab <- or_tab %>%
  mutate(
    OR = exp(median_log),
    OR_low = exp(lower95_log),
    OR_high = exp(upper95_log)
  )

# order for plotting (adjust to taste)
or_tab <- or_tab %>%
  arrange(OR) %>%
  mutate(label = factor(label, levels = label))   # keeps arranged order (bottom -> top)

# plot
p_or_med <- ggplot(or_tab, aes(x = OR, y = label)) +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = OR_low, xmax = OR_high), height = 0.25) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "grey40") +
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.1))) +
  labs(
    title = "Posterior Odds Ratios (Median, 95% CrI)",
    x = "Odds Ratio (Median) with 95% CrI",
    y = ""
  ) +
  theme_minimal(base_size = 13) +
  theme(panel.grid.minor = element_blank(), axis.text.y = element_text(size = 11))

print(p_or_med)

# save outputs
ggsave("posterior_odds_ratios_median_forest.png", p_or_med, width = 8, height = 6, dpi = 300)
write.csv(or_tab, "posterior_odds_ratios_median_table.csv", row.names = FALSE)

cat("Saved posterior plot to 'posterior_odds_ratios_median_forest.png' and table to 'posterior_odds_ratios_median_table.csv'\n")

```


```{r}
# ----------------------------
# Map JAGS parameter names -> human-friendly labels + relabel diagnostics
# ----------------------------

# Assumptions:
# - samples_matrix: matrix of posterior draws (as.matrix(samples))
# - param_order: vector of parameter names in samples_matrix in the order you used to build coef_samples
# - cov_names: column names of X (order matching beta[1:NCOV])
# - strokes: dataframe used in the model
# - K_race, K_tr, K_time and their level names are available

# 1) Build mapping
human_labels <- character(length = length(param_order))

for(i in seq_along(param_order)){
  p <- param_order[i]
  # intercept
  if(p == "beta0"){
    human_labels[i] <- "Intercept"
    next
  }
  # beta j -> covariate name (X columns)
  if(grepl("^beta\\[[0-9]+\\]", p)){
    j <- as.integer(gsub("^beta\\[([0-9]+)\\]$", "\\1", p))
    if(j <= length(cov_names)){
      human_labels[i] <- cov_names[j]
    } else {
      human_labels[i] <- paste0("beta[", j, "]") # fallback
    }
    next
  }
  # beta_age
  if(p == "beta_age"){
    human_labels[i] <- "Age"
    next
  }
  # beta_race[k]  (these are only for k = 2..K_race in param_order)
  if(grepl("^beta_race\\[", p)){
    k <- as.integer(gsub("^beta_race\\[([0-9]+)\\]$", "\\1", p))
    # get full level names; baseline in JAGS was beta_race[1] = 0
    race_levs <- levels(strokes$Race2)
    if(k >= 1 && k <= length(race_levs)){
      human_labels[i] <- paste0("Race2_", race_levs[k])
    } else {
      human_labels[i] <- p
    }
    next
  }
  # beta_tr[k]
  if(grepl("^beta_tr\\[", p)){
    k <- as.integer(gsub("^beta_tr\\[([0-9]+)\\]$", "\\1", p))
    tr_levs <- levels(strokes$TransportNotify)
    if(k >= 1 && k <= length(tr_levs)){
      human_labels[i] <- paste0("TransportNotify_", tr_levs[k])
    } else human_labels[i] <- p
    next
  }
  # beta_time[k]
  if(grepl("^beta_time\\[", p)){
    k <- as.integer(gsub("^beta_time\\[([0-9]+)\\]$", "\\1", p))
    time_levs <- levels(strokes$Time2)
    if(k >= 1 && k <= length(time_levs)){
      human_labels[i] <- paste0("Time2_", time_levs[k])
    } else human_labels[i] <- p
    next
  }
  # fallback
  human_labels[i] <- p
}

param_map <- data.frame(
  jags_param = param_order,
  human_label = human_labels,
  stringsAsFactors = FALSE
)

# print and save mapping
print(param_map)
write.csv(param_map, "jags_param_map.csv", row.names = FALSE)

# 2) Helpful lookup function to relabel any diagnostic table/plot
relabel_vec <- function(vec){
  # vec is character vector of jags param names
  idx <- match(vec, param_map$jags_param)
  lab <- param_map$human_label[idx]
  # where missing keep original
  lab[is.na(lab)] <- vec[is.na(lab)]
  return(lab)
}

# Example: relabel your numeric summaries (rhat_df, ess_df, param_summary)
if(exists("rhat_df") && nrow(rhat_df) > 0){
  rhat_df$Parameter_human <- relabel_vec(as.character(rhat_df$Parameter))
  rhat_df <- rhat_df %>% arrange(desc(Rhat))
  cat("\nTop R-hat (with human labels):\n")
  print(head(rhat_df %>% select(Parameter_human, Rhat, Rhat_upper), 30))
}

if(exists("ess_df") && nrow(ess_df) > 0){
  ess_df$Parameter_human <- relabel_vec(as.character(ess_df$Parameter))
  ess_df <- ess_df %>% arrange(ESS)
  cat("\nLowest ESS (with human labels):\n")
  print(head(ess_df %>% select(Parameter_human, ESS), 30))
}

# If you constructed param_summary earlier, relabel it for printing and plotting:
if(exists("param_summary")){
  param_summary$Parameter_human <- relabel_vec(as.character(param_summary$Parameter))
  # useful sort
  param_summary <- param_summary %>% arrange(ESS)
  write.csv(param_summary, "posterior_param_summary_with_human_labels.csv", row.names = FALSE)
}

# 3) Use human labels in ACF and trace plots
# The code that built acf_df and trace_df earlier used Parameter names from samples_mat.
# Replace/augment Parameter with the human label for plotting.

if(exists("acf_df")){
  acf_df$Parameter_human <- relabel_vec(as.character(acf_df$Parameter))
  # when plotting, use Parameter_human as facet variable:
  p_acf_human <- ggplot(acf_df, aes(x = Lag, y = ACF)) +
    geom_col() +
    geom_hline(yintercept = 0, color = "grey40") +
    facet_wrap(~ Parameter_human, scales = "free_y", ncol = 6) +
    labs(title = paste0("ACF (lags 1-", max(acf_df$Lag, na.rm=TRUE), ") — faceted (human labels)"),
         x = "Lag", y = "ACF") +
    theme_minimal(base_size = 10) +
    theme(strip.text = element_text(size = 8))
  print(p_acf_human)
}

if(exists("trace_df")){
  trace_df$Parameter_human <- relabel_vec(as.character(trace_df$Parameter))
  p_trace_human <- ggplot(trace_df, aes(x = iter, y = Value, color = chain)) +
    geom_line(alpha = 0.6) +
    facet_wrap(~ Parameter_human, scales = "free_y", ncol = 3) +
    labs(title = paste0("Traceplots (human labels) — ", length(unique(trace_df$Parameter_human)), " params"),
         x = "Iteration", y = "Value") +
    theme_minimal(base_size = 10) +
    theme(legend.position = "bottom", strip.text = element_text(size = 9))
  print(p_trace_human)
}

# For caterpillar: map site param names if you want to label them differently (these are numeric site indices)
if(exists("site_df")){
  # site_df already used param names like b_site_home[1]; you can keep or create nicer labels.
  site_df$site_label <- site_df$site
  p_cat_human <- ggplot(site_df, aes(x = mean, y = reorder(site_label, mean))) +
    geom_point() +
    geom_errorbarh(aes(xmin = low, xmax = high), height = 0.2) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "grey40") +
    labs(title = "Caterpillar: b_site_home (site index)", x = "Posterior mean (95% CI)", y = "site index") +
    theme_minimal(base_size = 10)
  print(p_cat_human)
}

# ----------------------------
# 4) Binned residual plot (observed minus posterior-mean predicted probability)
# ----------------------------

# Compute posterior means for parameters used in param_order
post_means <- colMeans(samples_matrix[, param_order, drop = FALSE])

# Re-create linear predictor for each observation using the same parameterization as the JAGS model
# Start with X*beta (X columns correspond to beta[1:NCOV])
beta_from_post <- post_means[grepl("^beta\\[[0-9]+\\]", names(post_means))]
# ensure ordering
beta_idx <- as.integer(gsub("^beta\\[([0-9]+)\\]$", "\\1", names(beta_from_post)))
beta_vec <- numeric(length = length(cov_names))
beta_vec[beta_idx] <- as.numeric(beta_from_post[order(beta_idx)])
# if lengths align, multiply; else warn
if(length(beta_vec) != ncol(X)){
  stop("beta vector length does not match ncol(X). Check cov_names and X ordering.")
}

lp_X <- as.numeric(X %*% beta_vec)

# add intercept (beta0)
if("beta0" %in% names(post_means)){
  lp <- post_means["beta0"] + lp_X
} else {
  lp <- lp_X
}

# add age effect
if("beta_age" %in% names(post_means)){
  lp <- lp + post_means["beta_age"] * jags_data$age
}

# add categorical effects: race, tr, time
# Build full vectors (including baseline = 0 at index 1)
if(K_race > 1){
  beta_race_means <- rep(0, K_race)
  # find param names like beta_race[2]...
  for(k in 2:K_race){
    nm <- paste0("beta_race[", k, "]")
    if(nm %in% names(post_means)) beta_race_means[k] <- post_means[nm]
  }
  lp <- lp + beta_race_means[jags_data$race]
}
if(K_tr > 1){
  beta_tr_means <- rep(0, K_tr)
  for(k in 2:K_tr){
    nm <- paste0("beta_tr[", k, "]")
    if(nm %in% names(post_means)) beta_tr_means[k] <- post_means[nm]
  }
  lp <- lp + beta_tr_means[jags_data$tr]
}
if(K_time > 1){
  beta_time_means <- rep(0, K_time)
  for(k in 2:K_time){
    nm <- paste0("beta_time[", k, "]")
    if(nm %in% names(post_means)) beta_time_means[k] <- post_means[nm]
  }
  lp <- lp + beta_time_means[jags_data$time]
}

# site random effects: use posterior means if present (b_site_home[s])
site_params <- grep("^b_site_home\\[", colnames(samples_matrix), value = TRUE)
if(length(site_params) > 0){
  b_site_means <- colMeans(samples_matrix[, site_params, drop = FALSE])
  # extract indices
  site_idx_vec <- as.integer(gsub("^b_site_home\\[([0-9]+)\\]$", "\\1", names(b_site_means)))
  b_site_full <- numeric(nsite)
  b_site_full[site_idx_vec] <- b_site_means[order(site_idx_vec)]
  lp <- lp + b_site_full[jags_data$site]
}

# predicted probability (posterior-mean linear predictor -> probability)
p_hat <- plogis(lp)
y_obs <- jags_data$Y

# residuals: raw Pearson residuals (Y - p_hat)
residuals_raw <- y_obs - p_hat

# create bins (default 10 equal-width bins by p_hat)
nbins <- 30
bin_cut <- quantile(p_hat, probs = seq(0, 1, length.out = nbins + 1), na.rm = TRUE)
# ensure unique breaks (handles ties)
bin_cut <- unique(bin_cut)
if(length(bin_cut) - 1 < nbins) {
  # fallback to pretty equal-width
  bin_cut <- seq(0, 1, length.out = nbins + 1)
}
bin_id <- cut(p_hat, breaks = bin_cut, include.lowest = TRUE, labels = FALSE)

binned <- data.frame(
  bin = bin_id,
  p_hat = p_hat,
  resid = residuals_raw
) %>%
  group_by(bin) %>%
  summarize(
    mean_p = mean(p_hat, na.rm = TRUE),
    mean_resid = mean(resid, na.rm = TRUE),
    sd_resid = sd(resid, na.rm = TRUE),
    n = n(),
    se_resid = sd_resid / sqrt(n),
    lower95 = mean_resid - 1.96 * se_resid,
    upper95 = mean_resid + 1.96 * se_resid,
    .groups = "drop"
  ) %>%
  arrange(mean_p)

# plot binned residuals
p_binned <- ggplot(binned, aes(x = mean_p, y = mean_resid)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = lower95, ymax = upper95), width = 0.02) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey40") +
  labs(
    title = paste0("Binned residual plot for Bayesian model\n(", nbins, " bins): observed - predicted"),
    x = "Mean predicted probability in bin",
    y = "Mean residual (Y - p̂) with 95% CI"
  ) +
  theme_minimal(base_size = 12)

print(p_binned)

# Save binned residuals table
write.csv(binned, "binned_residuals_table.csv", row.names = FALSE)

cat("\nSaved mapping (jags_param_map.csv) and posterior summaries with human labels.\nBinned residual plot produced and binned_residuals_table.csv saved.\n")

```


```{r}
# Focused diagnostics: Rhat, ESS, ACF (single faceted plot), Trace grid, Pairs, Caterpillar, Posterior table

# ---------- helpers ----------
as_matrix_draws <- function(mcl) as.matrix(do.call(rbind, lapply(mcl, as.matrix)))
safe_gelman <- function(mcl){
  out <- tryCatch(gelman.diag(mcl, multivariate = FALSE),
                  error = function(e) { message("gelman.diag error: ", e$message); return(NULL) })
  return(out)
}

# ---------- basic info ----------
samples_mat <- as_matrix_draws(samples)
param_names <- colnames(samples_mat)
n_params <- length(param_names)
cat("Chains:", length(samples), " Total draws (rows):", nrow(samples_mat), " Parameters:", n_params, "\n\n")

# ---------- 1) R-hat ----------
gel <- safe_gelman(samples)
if(!is.null(gel)){
  # older coda may name columns slightly differently; handle both
  psrf <- as.data.frame(gel$psrf)
  colnames(psrf) <- make.names(colnames(psrf))
  rhat_df <- data.frame(Parameter = rownames(psrf),
                        Rhat = psrf[[1]],
                        Rhat_upper = if(ncol(psrf) >= 2) psrf[[2]] else NA)
  rhat_df <- rhat_df %>% arrange(desc(Rhat))
  cat("Top R-hat (largest first):\n")
  print(head(rhat_df, 30))
} else {
  rhat_df <- data.frame(Parameter = character(0), Rhat = numeric(0), Rhat_upper = numeric(0))
  message("R-hat skipped (gelman.diag failed).")
}

# ---------- 2) ESS ----------
ess_vals <- effectiveSize(samples)
ess_df <- data.frame(Parameter = names(ess_vals), ESS = as.numeric(ess_vals))
ess_df <- ess_df %>% arrange(ESS)
cat("\nLowest ESS (showing up to 30):\n")
print(head(ess_df, 30))

# helper: choose parameters to plot (prioritize problematic ones)
pick_params_for_plots <- function(all_params, max_panels = 36){
  if(length(all_params) <= max_panels) return(all_params)
  # pick union of lowest ESS and highest Rhat
  low_ess <- ess_df$Parameter[1:min(nrow(ess_df), max_panels)]
  high_rhat <- if(nrow(rhat_df)>0) rhat_df$Parameter[1:min(nrow(rhat_df), max_panels)] else character(0)
  sel <- unique(c(low_ess, high_rhat))
  if(length(sel) > max_panels) sel <- sel[1:max_panels]
  return(sel)
}

# ---------- 3) ACF faceted plot (single multi-panel ggplot) ----------
# compute acf (lags 0..30) on combined draws for each parameter (or selection)
max_lag <- 30
acf_candidates <- pick_params_for_plots(param_names, max_panels = 36)  # up to 36 panels
cat("\nACF panels will show", length(acf_candidates), "parameters (selected for readability).\n")

acf_list <- lapply(acf_candidates, function(p){
  vals <- samples_mat[, p]
  # use stats::acf on the combined series (no plotting)
  a <- acf(vals, plot = FALSE, lag.max = max_lag)
  data.frame(Parameter = p,
             Lag = as.integer(a$lag[,1,1]),
             ACF = as.numeric(a$acf[,1,1]),
             stringsAsFactors = FALSE)
})
acf_df <- bind_rows(acf_list)
# remove lag 0 if you prefer (keep if you want correlation at lag0)
acf_df <- acf_df %>% filter(Lag > 0)

# make faceted bar chart
p_acf <- ggplot(acf_df, aes(x = Lag, y = ACF)) +
  geom_col() +
  geom_hline(yintercept = 0, color = "grey40") +
  facet_wrap(~ Parameter, scales = "free_y", ncol = 6) +
  labs(title = paste0("ACF (lags 1-", max_lag, ") — faceted panels (", length(acf_candidates), " params)"),
       x = "Lag", y = "ACF") +
  theme_minimal(base_size = 10) +
  theme(strip.text = element_text(size = 8))
print(p_acf)

# ---------- 4) Traceplots (chains overlaid) in grid (ggplot facets) ----------
# We'll build a long dataframe with chain and iteration info
mcmc_long <- bind_rows(lapply(seq_along(samples), function(ci){
  mat <- as.matrix(samples[[ci]])
  df <- as.data.frame(mat)
  df$iter <- seq_len(nrow(df))
  df$chain <- paste0("chain", ci)
  # pivot longer
  df_long <- pivot_longer(df, cols = -c(iter, chain), names_to = "Parameter", values_to = "Value")
  return(df_long)
}), .id = NULL)

# choose params for traces (<=24 preferred to keep readable)
trace_candidates <- pick_params_for_plots(param_names, max_panels = 24)
cat("Traceplot panels will show", length(trace_candidates), "parameters (selected for readability).\n")

trace_df <- mcmc_long %>% filter(Parameter %in% trace_candidates)

p_trace <- ggplot(trace_df, aes(x = iter, y = Value, color = chain)) +
  geom_line(alpha = 0.6) +
  facet_wrap(~ Parameter, scales = "free_y", ncol = 3) +
  labs(title = paste0("Traceplots (chains overlaid) — faceted, ", length(trace_candidates), " params"),
       x = "Iteration", y = "Value") +
  theme_minimal(base_size = 10) +
  theme(legend.position = "bottom", strip.text = element_text(size = 9))
print(p_trace)

# ---------- 5) Pairs plot (small selected set) ----------
pairs_candidates <- c("beta0",
                      paste0("beta[", 1:min(4, NCOV), "]"),
                      "beta_age",
                      paste0("beta_race[", 2:min(K_race, 2), "]"),
                      paste0("beta_tr[", 2:min(K_tr, 2), "]"),
                      paste0("beta_time[", 2:min(K_time, 2), "]"))
pairs_params <- intersect(param_names, pairs_candidates)
if(length(pairs_params) >= 2){
  cat("\nPairs plot for selected params:\n")
  print(pairs_params)
  pairs(as.matrix(samples)[, pairs_params], main = "Pairs: selected parameters")
} else {
  message("Not enough selected parameters available for pairs plot.")
}

# ---------- 6) Site random effects caterpillar ----------
site_params <- grep("^b_site_home\\[", param_names, value = TRUE)
if(length(site_params) > 0){
  site_mat <- samples_mat[, site_params, drop = FALSE]
  site_mean <- apply(site_mat, 2, mean)
  site_low <- apply(site_mat, 2, quantile, .025)
  site_high <- apply(site_mat, 2, quantile, .975)
  site_df <- data.frame(site = seq_along(site_params),
                        param = site_params,
                        mean = site_mean,
                        low = site_low,
                        high = site_high,
                        stringsAsFactors = FALSE) %>%
    arrange(mean)
  # ggplot caterpillar
  p_cat <- ggplot(site_df, aes(x = mean, y = reorder(param, mean))) +
    geom_point() +
    geom_errorbarh(aes(xmin = low, xmax = high), height = 0.2) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "grey40") +
    labs(title = "Caterpillar: b_site_home", x = "Posterior mean (95% CI)", y = "site (param)") +
    theme_minimal(base_size = 10)
  print(p_cat)
} else {
  message("No b_site_home[...] parameters found for caterpillar.")
}

# ---------- 7) Posterior numeric summary (concise) ----------
param_summary <- data.frame(
  Parameter = param_names,
  Mean = round(apply(samples_mat, 2, mean), 4),
  Median = round(apply(samples_mat, 2, median), 4),
  SD = round(apply(samples_mat, 2, sd), 4),
  `2.5%` = round(apply(samples_mat, 2, quantile, 0.025), 4),
  `97.5%` = round(apply(samples_mat, 2, quantile, 0.975), 4),
  ESS = round(as.numeric(effectiveSize(samples)), 1),
  stringsAsFactors = FALSE
)
cat("\nPosterior summary (showing 40 parameters sorted by ESS asc):\n")
print(head(param_summary[order(param_summary$ESS), ], 40))

# ---------- Problems summary ----------
problem_rhat <- if(nrow(rhat_df)>0) rhat_df$Parameter[rhat_df$Rhat > 1.1] else character(0)
problem_ess  <- ess_df$Parameter[ess_df$ESS < 200]
problematic <- unique(c(problem_rhat, problem_ess))
cat("\nProblematic parameters (Rhat>1.1 or ESS<200):\n")
print(problematic)

cat("\nDone. All plots drawn to the active plotting device. If you want fewer/more panels or different selections (e.g., only parameters with ESS<200), tell me and I'll update the selection logic.\n")

```

```{r}
# ----------------------------
# Posterior OR plot (forest) using posterior draws already in samples_matrix
# - place after you have `samples_matrix`, `param_order`, `cov_names`, `strokes`
# ----------------------------

library(dplyr)
library(ggplot2)

# 1) If param_map doesn't already exist, build a mapping (same logic as earlier)
if(!exists("param_map")){
  human_labels <- character(length = length(param_order))
  for(i in seq_along(param_order)){
    p <- param_order[i]
    if(p == "beta0"){
      human_labels[i] <- "Intercept"
      next
    }
    if(grepl("^beta\\[[0-9]+\\]", p)){
      j <- as.integer(gsub("^beta\\[([0-9]+)\\]$", "\\1", p))
      human_labels[i] <- if(j <= length(cov_names)) cov_names[j] else p
      next
    }
    if(p == "beta_age"){
      human_labels[i] <- "Age"
      next
    }
    if(grepl("^beta_race\\[", p)){
      k <- as.integer(gsub("^beta_race\\[([0-9]+)\\]$", "\\1", p))
      levs <- levels(strokes$Race2)
      human_labels[i] <- if(k >= 1 && k <= length(levs)) paste0("Race2_", levs[k]) else p
      next
    }
    if(grepl("^beta_tr\\[", p)){
      k <- as.integer(gsub("^beta_tr\\[([0-9]+)\\]$", "\\1", p))
      levs <- levels(strokes$TransportNotify)
      human_labels[i] <- if(k >= 1 && k <= length(levs)) paste0("TransportNotify_", levs[k]) else p
      next
    }
    if(grepl("^beta_time\\[", p)){
      k <- as.integer(gsub("^beta_time\\[([0-9]+)\\]$", "\\1", p))
      levs <- levels(strokes$Time2)
      human_labels[i] <- if(k >= 1 && k <= length(levs)) paste0("Time2", levs[k]) else p
      next
    }
    # fallback
    human_labels[i] <- p
  }
  param_map <- data.frame(jags_param = param_order, human_label = human_labels, stringsAsFactors = FALSE)
  write.csv(param_map, "jags_param_map_for_forest.csv", row.names = FALSE)
}

# 2) Posterior summaries (log-odds)
# Use samples_matrix and param_order to compute mean/median/CI on the log-odds scale
if(!("samples_matrix" %in% ls())){
  stop("samples_matrix not found. Create it with samples_matrix <- as.matrix(samples) or similar before running this block.")
}

# make sure param_order are present in samples_matrix columns
present_params <- intersect(param_order, colnames(samples_matrix))
if(length(present_params) == 0) stop("No parameters from param_order found in samples_matrix.")

summ_list <- lapply(present_params, function(p){
  draws <- samples_matrix[, p]
  data.frame(
    jags_param = p,
    mean_log = mean(draws, na.rm = TRUE),
    median_log = median(draws, na.rm = TRUE),
    lower95_log = quantile(draws, 0.025, na.rm = TRUE),
    upper95_log = quantile(draws, 0.975, na.rm = TRUE),
    stringsAsFactors = FALSE
  )
}) %>% bind_rows()

# attach human labels
summ <- summ_list %>%
  left_join(param_map, by = "jags_param")

# 3) Keep only fixed-effects coefficients (drop intercept & random effects like b_site_home[...])
# Remove intercept and any params that look like random effects (b_site_home[)
summ <- summ %>%
  filter(jags_param != "beta0") %>%                      # remove intercept
  filter(!grepl("^b_site_home\\[", jags_param))          # remove site random effects

# Optionally drop any other parameters you don't want (e.g. any hyperparameters)
# e.g. filter(!grepl("tau_|pi_|beta_race\\[1\\]", jags_param))

# 4) Convert to odds ratios
summ <- summ %>%
  mutate(
    OR = exp(median_log),            # median on log-odds -> OR
    OR_low = exp(lower95_log),
    OR_high = exp(upper95_log),
    label = human_label
  )

# If there are duplicate labels (e.g., multiple coefficients map to same human label), disambiguate:
if(any(duplicated(summ$label))){
  summ <- summ %>%
    mutate(label = ifelse(duplicated(label), paste0(label, " (", jags_param, ")"), label))
}

# 5) Order factors so the largest OR is at top (like your screenshot)
summ <- summ %>%
  arrange(OR) %>%
  mutate(label = factor(label, levels = label))

# 6) Plot: horizontal forest of OR with 95% CrI
p_or <- ggplot(summ, aes(x = OR, y = label)) +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = OR_low, xmax = OR_high), height = 0.25) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "grey40") +
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.1))) +
  labs(
    title = "Posterior Odds Ratios (Median, 95% CrI)",
    x = "Odds Ratio (Median) with 95% CrI",
    y = ""
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    axis.text.y = element_text(size = 11)
  )

# print to device
print(p_or)

# Save the plot and the summary table
ggsave("posterior_odds_ratios_forest.png", p_or, width = 9, height = 6, dpi = 300)
write.csv(summ, "posterior_odds_ratios_table.csv", row.names = FALSE)

cat("Saved 'posterior_odds_ratios_forest.png' and 'posterior_odds_ratios_table.csv'\n")

```



# MODIFIED IMPUTATION

```{r}
library(rjags)

# --- Step 1: Preprocess data ---
strokes$Age[strokes$Age == ""] <- NA
strokes$EMSvsCar[strokes$EMSvsCar == ""] <- NA
strokes$PreHospNotify[strokes$PreHospNotify == ""] <- NA
strokes$Race2[strokes$Race2 == "Missing"] <- NA
strokes$Race2 <- factor(strokes$Race2)
strokes <- strokes |> 
  mutate(Time2 = case_when(Time2 == "Y1Q1" | Time2 == "Y1Q2" ~ 1,
                           Time2 == "Y2Q3" | Time2 == "Y2Q4" ~ 3,
                           TRUE ~ 2))
# Create TransportNotify variable
strokes$TransportNotify <- with(
  strokes,
  ifelse(EMSvsCar == 0, 1,
         ifelse(PreHospNotify == 0, 2, 3))
)
strokes$TransportNotify <- as.numeric(strokes$TransportNotify)

# Convert site to numeric
strokes$siteID <- as.numeric(factor(strokes$siteID))

# --- Step 2: Select covariates automatically ---
outcomes <- c("Age", "TransportNotify", "Race2", "siteID")
covariate_names <- grep(paste(outcomes, collapse="|"), 
                        names(strokes), 
                        invert = TRUE, 
                        value = TRUE)

# Convert factors/logicals to numeric and handle NAs
X <- strokes[, covariate_names]
X <- data.frame(lapply(X, function(x) {
  if(is.factor(x) | is.logical(x)) x <- as.numeric(x)
  x[is.na(x)] <- 0  # temporary placeholder
  return(x)
}))
X <- as.matrix(X)
NCOV <- ncol(X)

# --- Step 3: Prepare data for JAGS ---
jags_data <- list(
  N = nrow(strokes),
  age = strokes$Age,
  tr = strokes$TransportNotify,
  race = as.numeric(strokes$Race2),
  site = strokes$siteID,
  nsite = length(unique(strokes$siteID)),
  K_tr = 3,
  K_race = length(levels(strokes$Race2)),
  NCOV = NCOV,
  X = X,
  alpha = rep(0.5, length(levels(strokes$Race2))),
  Y = as.integer(strokes$homeOrRehab)
)

# --- Step 4: Write JAGS model ---
model_string <- "
model {

  #### AGE MODEL (regression + site random effect) ####
  for (i in 1:N) {
    age[i] ~ dnorm(mu_age[i], tau_age)
    mu_age[i] <- beta0 + inprod(beta[], X[i,]) + b_site[site[i]]
  }
  tau_age ~ dgamma(0.01,0.01)
  sigma_age <- 1/sqrt(tau_age)

  for (s in 1:nsite) {
    b_site[s] ~ dnorm(0, tau_b)
  }
  tau_b ~ dgamma(0.01,0.01)
  sigma_b <- 1/sqrt(tau_b)

  for (j in 1:NCOV) {
    beta[j] ~ dnorm(0,0.001)
  }
  beta0 ~ dnorm(0,0.001)

  #### TRANSPORT-NOTIFY MODEL (multinomial logistic regression + site random effects) ####
  for (i in 1:N) {
    tr[i] ~ dcat(pi_tr[i,1:K_tr])
    for (k in 1:K_tr) {
      logit(pi_tr[i,k]) <- alpha_tr[k] + inprod(gamma[k,], X[i,]) + u_site[site[i],k]
    }
  }

  for (s in 1:nsite) {
    for (k in 1:K_tr) {
      u_site[s,k] ~ dnorm(0, tau_u)
    }
  }
  tau_u ~ dgamma(0.01,0.01)

  for (k in 1:K_tr) {
    for (j in 1:NCOV) {
      gamma[k,j] ~ dnorm(0,0.001)
    }
    alpha_tr[k] ~ dnorm(0,0.001)
  }

  #### RACE MODEL (site-only) ####
  for (i in 1:N) {
    race[i] ~ dcat(pi_race[site[i],1:K_race])
  }

  for (s in 1:nsite) {
    pi_race[s,1:K_race] ~ ddirch(alpha[1:K_race])
  }
}
"

writeLines(model_string, con = "jags_impute_model.txt")

# --- Step 5: Set initial values ---
inits_list <- list(
  list(beta0=50, beta=rep(0,NCOV), tau_age=1, tau_b=1,
       u_site=matrix(0,nrow=jags_data$nsite,ncol=jags_data$K_tr),
       gamma=matrix(0,jags_data$K_tr,NCOV), alpha_tr=rep(0,jags_data$K_tr)
       ),
  list(beta0=40, beta=rep(0,NCOV), tau_age=1, tau_b=50,
       u_site=matrix(0,nrow=jags_data$nsite,ncol=jags_data$K_tr),
       gamma=matrix(0,jags_data$K_tr,NCOV), alpha_tr=rep(0,jags_data$K_tr)),
  list(beta0=60, beta=rep(0,NCOV), tau_age=1, tau_b=15,
       u_site=matrix(0,nrow=jags_data$nsite,ncol=jags_data$K_tr),
       gamma=matrix(0,jags_data$K_tr,NCOV), alpha_tr=rep(0,jags_data$K_tr))
)

# --- Step 6: Parameters to monitor ---
params <- c("age", "mu_age", "tr", "race", "beta0", "beta", "tau_age", "b_site", "tau_b", "gamma", "alpha_tr", "u_site", "pi_race")


# --- Step 7: Run JAGS ---
jmod <- jags.model("jags_impute_model.txt", data=jags_data, inits=inits_list,
                   n.chains=3, n.adapt=1000)
update(jmod, 2000)
samples <- coda.samples(jmod, variable.names=params, n.iter=20000)

# --- Step 8: Inspect samples ---
# samples

samp <- saveRDS(samples, "samples")

```
```{r}

samples <- readRDS("samples")

library(coda)

# --- Extract hyperparameters from the posterior ---
# Assuming 'samples' is a coda mcmc.list
samples_hyper <- samples[, c("beta0", "tau_age", "tau_b")]

# --- Gelman-Rubin diagnostic ---
gelman.diag(samples_hyper)

# --- Traceplots ---
traceplot(samples[, c("beta0", "tau_age", "tau_b")])

# --- Effective sample size ---
effectiveSize(samples[, c("beta0", "tau_age", "tau_b")])


```
```{r}
library(dplyr)
library(coda)

# ============================================================
# STEP 0 — PREPARE DATA + POSTERIOR MATRIX
# ============================================================

post_mat <- as.matrix(samples)  # convert coda object to matrix
N <- nrow(strokes)

# Identify missing data
missing_age  <- which(is.na(strokes$Age))
missing_tr   <- which(is.na(strokes$TransportNotify))

# ---- Prepare Race2: convert to numeric codes for safe imputation ----
race_levels <- levels(strokes$Race2)
strokes$Race2_num <- as.numeric(strokes$Race2)
missing_race <- which(is.na(strokes$Race2_num))
K_race <- length(race_levels)

# ---- Site indexing ----
sites  <- unique(strokes$siteID)
Nsites <- length(sites)
site_index <- match(strokes$siteID, sites)

# ---- Posterior parameter name groups ----
b_names  <- grep("^b_site\\[",  colnames(post_mat), value=TRUE)
u_names  <- grep("^u_site\\[",  colnames(post_mat), value=TRUE)
pi_names <- grep("^pi_race\\[", colnames(post_mat), value=TRUE)

NCOV <- ncol(X)
K_tr <- 3  # transport categories

# Number of imputed datasets
m <- 20
imputed_datasets <- vector("list", m)


# ============================================================
# STEP 1 — IMPUTATION LOOP
# ============================================================

set.seed(123)

for (j in 1:m) {

  iter <- sample(seq_len(nrow(post_mat)), 1)
  post <- post_mat[iter, ]

  imputed_data <- strokes


  # ============================================================
  # 1. IMPUTE AGE (Normal regression w/ site random effects)
  # ============================================================

  mu_age <- rep(post["beta0"], N)

  # fixed effects
  beta_idx <- paste0("beta[", 1:NCOV, "]")
  mu_age <- mu_age + as.numeric(X %*% post[beta_idx])

  # site random effects
  b_vals <- post[b_names]
  if (length(b_vals) != Nsites)
      stop("Dimension mismatch in b_site")

  mu_age <- mu_age + b_vals[site_index]

  # draw age imputations
  tau_age <- post["tau_age"]
  imputed_data$Age[missing_age] <- rnorm(
    length(missing_age),
    mean = mu_age[missing_age],
    sd   = 1/sqrt(tau_age)
  )


  # ============================================================
  # 2. IMPUTE TRANSPORT NOTIFY (Multinomial logistic)
  # ============================================================

  # alpha_tr[k]
  alpha_tr <- post[paste0("alpha_tr[", 1:K_tr, "]")]

  # gamma[k,]
  gamma_mat <- matrix(NA, nrow=K_tr, ncol=NCOV)
  for (k in 1:K_tr) {
    gamma_mat[k, ] <- post[grep(
      paste0("^gamma\\[", k, ","),
      names(post)
    )]
  }

  # u_site[site,k] matrix
  u_mat <- matrix(post[u_names], nrow=Nsites, ncol=K_tr, byrow=TRUE)

  # logits (N × K_tr)
  logits <- matrix(0, nrow=N, ncol=K_tr)
  for (k in 1:K_tr) {
    logits[, k] <- alpha_tr[k] + X %*% gamma_mat[k, ] + u_mat[site_index, k]
  }

  # stable softmax
  logits_c <- logits - apply(logits, 1, max)
  exp_logits <- exp(logits_c)
  tr_probs <- exp_logits / rowSums(exp_logits)

  # impute transport notify
  for (i in missing_tr) {
    imputed_data$TransportNotify[i] <-
      sample(1:K_tr, 1, prob = tr_probs[i, ])
  }


  # ============================================================
  # 3. IMPUTE RACE2 (site-specific categorical)
  # ============================================================

  # reshape pi_race to (site × race)
  pi_mat <- matrix(
    post[pi_names],
    nrow = Nsites,
    ncol = K_race,
    byrow = TRUE
  )

  # safety: no negative or NA probabilities
  if (any(is.na(pi_mat)))
      stop("NA in posterior pi_race matrix.")
  pi_mat <- abs(pi_mat)
  pi_mat <- pi_mat / rowSums(pi_mat)

  # impute numeric race category
  for (i in missing_race) {
    s_row <- site_index[i]
    probs <- pi_mat[s_row, ]
    imputed_data$Race2_num[i] <- sample(seq_len(K_race), 1, prob = probs)
  }


  # store dataset
  imputed_datasets[[j]] <- imputed_data
}


# ============================================================
# STEP 2 — CONVERT RACE BACK TO FACTOR IN ALL DATASETS
# ============================================================

for (j in 1:m) {
  imputed_datasets[[j]]$Race2 <- factor(
    race_levels[ imputed_datasets[[j]]$Race2_num ],
    levels = race_levels
  )
}

saveRDS(imputed_datasets, "imputed_data")
```






```{r}
library(ggplot2)
library(coda)
library(dplyr)
library(tidyr)

# --- Step 1: Convert posterior to matrix ---
posterior_samples <- as.matrix(samples)

# --- Step 2: Exclude imputed variables ---
exclude_params <- c("^age", "^tr", "^race")
param_names <- colnames(posterior_samples)
param_names <- param_names[!grepl(paste(exclude_params, collapse="|"), param_names)]
# After excluding imputed variables, choose which parameters to plot
relevant_params <- c("beta0", "beta_age", "beta_race", "tau_age", "tau_b")

param_names <- param_names[param_names %in% relevant_params]


# --- Step 3: Prepare a long-format data frame for posterior ---
posterior_df <- as.data.frame(posterior_samples[, param_names])
posterior_long <- posterior_df %>%
  pivot_longer(everything(), names_to = "parameter", values_to = "posterior")

# --- Step 4: Compute prior densities ---
prior_list <- lapply(param_names, function(p) {
  
  # default prior values from your JAGS model
  if(grepl("^beta", p) | grepl("^gamma", p)) {
    mean0 <- 0
    sd0 <- sqrt(1/0.001)
  } else if(grepl("^beta0", p) | grepl("^alpha_tr", p)) {
    mean0 <- 0
    sd0 <- sqrt(1/0.001)
  } else if(grepl("^tau", p)) {
    # tau ~ dgamma(0.01,0.01)
    # approximate prior density using rgamma
    x_seq <- seq(0, max(posterior_samples[,p])*2, length.out = 1000)
    density_vals <- dgamma(x_seq, shape = 0.01, rate = 0.01)
    return(data.frame(x=x_seq, density=density_vals, parameter=p, type="Prior"))
  } else if(grepl("^sigma", p)) {
    # transform tau to sigma
    x_seq <- seq(0, max(posterior_samples[,p])*2, length.out = 1000)
    density_vals <- dgamma(1/(x_seq^2), shape=0.01, rate=0.01) * 2/(x_seq^3)
    return(data.frame(x=x_seq, density=density_vals, parameter=p, type="Prior"))
  } else {
    # default fallback prior
    mean0 <- 0
    sd0 <- sqrt(1/0.001)
  }
  
  x_seq <- seq(min(posterior_samples[,p]), max(posterior_samples[,p]), length.out=1000)
  density_vals <- dnorm(x_seq, mean=mean0, sd=sd0)
  data.frame(x=x_seq, density=density_vals, parameter=p, type="Prior")
})

prior_df <- do.call(rbind, prior_list)

# --- Step 5: Combine posterior as density estimates ---
posterior_density <- posterior_long %>%
  group_by(parameter) %>%
  summarise(posterior_density = list(density(posterior)$y),
            posterior_x = list(density(posterior)$x)) %>%
  unnest(c(posterior_x, posterior_density)) %>%
  mutate(type="Posterior")

# --- Step 6: Combine prior and posterior ---
plot_df <- bind_rows(
  prior_df %>% rename(x=x, density=density),
  posterior_density %>% rename(x=posterior_x, density=posterior_density)
)

# --- Step 7: Plot ---
ggplot(plot_df, aes(x=x, y=density, color=type)) +
  geom_line(size=1) +
  facet_wrap(~parameter, scales="free") +
  scale_color_manual(values=c("Prior"="blue","Posterior"="red")) +
  labs(title="Prior vs Posterior for All Parameters (excluding imputed variables)",
       x="Parameter value", y="Density") +
  theme_minimal() +
  theme(legend.position="top")

```




```{r}
mu_cols <- grep("mu_age", colnames(samples[[1]]))
mu_draws <- as.matrix(samples)[, mu_cols]
mu_hat <- colMeans(mu_draws)
resid <- strokes$Age - mu_hat

# Residual vs fitted
plot(mu_hat, resid,
     xlab="Fitted Age", ylab="Residuals",
     main="Residuals vs Fitted")
abline(h=0, col="red")

# Check for non-constant variance
plot(mu_hat, abs(resid),
     xlab="Fitted Age", ylab="|Residual|",
     main="Heteroskedasticity Check")

```


# FREQUENTIST ANALYSIS

```{r}
library(pROC)
library(lme4)
results = NULL
AUCs = NULL
test <- readRDS("imputed_data")
for (i in 1:20){
  data <- test[[i]]
  data$homeOrRehab <- if_else(data$homeOrRehab,1,0)
  modelFit <- glm(factor(homeOrRehab) ~ ., data = data, family = "binomial")
  AUCs <- c(AUCs,auc(data$homeOrRehab,predict(modelFit,data)))
  results <- c(results, list(summary(modelFit)))
}
arm::binnedplot(fitted(modelFit),residuals(modelFit))
plot.roc(data$homeOrRehab,predict(modelFit,data))
results[[1]]$coefficients |> 
  as.data.frame() |> 
  select(-`z value`) |> 
  kableExtra::kable()
data.frame(iter = seq(1,20), AUC = AUCs) |> 
  kableExtra::kable()
```

# BAYESIAN ANALYSIS
{{< pagebreak >}}
```{r}
dataImputed <- readRDS("imputed_data")
library(rstan)
alpha <- beta <- tau <- r_hat <- n_eff <- Preds <- NULL
n_chains <- 2
for (i in 1:20){
data <- dataImputed[[i]]
data <- data |> 
  mutate(Time2 = case_when(Time2 == "Y1Q1" | Time2 == "Y1Q2" ~ 1,
                           Time2 == "Y2Q3" | Time2 == "Y2Q4" ~ 3,
                           TRUE ~ 2),
         Time2 = as.factor(Time2)) |> 
  select(-Race2_num,-EMSvsCar,-PreHospNotify)
Y <- as.integer(data$homeOrRehab)
X <- model.matrix(~.,data =data[2:10])[,-1]
X <- X[,-9]
Ids <- as.numeric(data$siteID)
n <- length(unique(Ids))
N <- nrow(X)
p <- ncol(X)
stan_data <- list(
  Y = Y,
  X = X,
  Ids = Ids,
  n = n,
  N = N,
  p = p
)
compiled_model <- stan_model(file = "logRegRE.stan")
options(mc.cores = 4)
fit_mi <- sampling(compiled_model, data = stan_data)

###Save convergence diagnostics from each imputed dataset
  r_hat <- cbind(r_hat, summary(fit_mi)$summary[, "Rhat"])
  n_eff <- cbind(n_eff, summary(fit_mi)$summary[, "n_eff"])
  pars <- rstan::extract(fit_mi, pars = c("alpha", "beta", "tau"))
  Y_pred <- rstan::extract(fit_mi, pars = "Y_pred")$Y_pred
  ### Save the parameters from each imputed dataset
  n_sims_chain <- length(pars$alpha) / n_chains
  alpha <- rbind(alpha, cbind(i, rep(1:n_chains, each = n_sims_chain), pars$alpha))
  beta <- rbind(beta, cbind(i, rep(1:n_chains, each = n_sims_chain), pars$beta))
  tau <- rbind(tau, cbind(i, rep(1:n_chains, each = n_sims_chain), pars$tau))
  Preds <-  rbind(Preds, cbind(i, rep(1:n_chains, each = n_sims_chain), Y_pred))
}  
saveRDS(alpha,"alphas.RDS")
saveRDS(beta,"betas.RDS")
saveRDS(tau,"taus.RDS")
saveRDS(Preds,"Y_preds.RDS")
coda::effectiveSize(beta)
library(bayesplot)
Y_pred <- rstan::extract(fit, pars = "Y_pred")$Y_pred
ppc_bars(Y, Y_pred[1:100, ])
ppc_bars_grouped(Y, Y_pred[1:100, ],data$Time2)
traceplot(fit, pars = c("alpha","beta"))
mcmc_acf(fit, regex_pars = c("alpha","beta"))
```



```{r}
Y <- as.integer(strokes$homeOrRehab)
#strokes <- strokes |> 
#  select(-EMSvsCar,-PreHospNotify) 
strokes$Age[is.na(strokes$Age)] <- -99
strokes$Race2 <- as.character(strokes$Race2)
strokes$Race2[is.na(strokes$Race2)] <- "XMissing"
strokes$Race2 <- as.factor(strokes$Race2)
strokes$TransportNotify[is.na(strokes$TransportNotify)] <- -99
X <- model.matrix(~.,data =strokes[2:10])[,-1]
X <- X[,-14]
Ids <- as.numeric(data$siteID)
n <- length(unique(Ids))
N <- nrow(X)
p <- ncol(X)
stan_data <- list(
  Y = Y,
  X = X,
  Ids = Ids,
  n = n,
  N = N,
  p = p
)
```

